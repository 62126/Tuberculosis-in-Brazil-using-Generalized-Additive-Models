---
title: "MTHM506Final"
author: "Chatchanok Arsuwate"
date: "2023-03-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(mgcv)
load('/Users/chatchanokarsuwate/Downloads/datasets_project.RData')
```

```{r}
 # Loading packages
library(fields)
library(maps)
library(sp)
# Install and load the broom package
#if (!requireNamespace("broom", quietly = TRUE)) {
#  install.packages("broom")
#}
library(broom)
library(tidyverse)
```
## our professor mention that we comprise at most 6 figures and 3 tables

## so I'll count 0/6 fig 0/3 table

We can start by looking at the summary statistics of the data:

this should be our first table in our report

## 0/6 fig 1/3 table

```{r}
summary(TBdata)
```


By checking the data attribute, the data type is divided into integer and numeric, so there is no need to do data type conversion. Through the summary data, there is no obvious abnormal data and missing data.

### Pre-analysis and pre-processing

## pick our first figure from this below option

## 1/6 fig 1/3 table

## option 1

These scatterplots will help us to identify any relationships between the covariates and the response variable. We can also use correlation matrices and scatterplot matrices to examine the relationships between all pairs of variables:

```{r}
cor(TBdata[,1:10])
pairs(TBdata[,1:10])
```

## option 2

The correlation diagram drawn by ggpairs can view data distribution and correlation

```{r}
library(ggcorrplot)
# Data cleaning 
# We use the correlation matrix to see the correlation coefficient, whether there is a correlation, and whether it is significant.
corr <- TBdata[, c(1:8)] %>%
  mutate(select(TBdata, TB))
corr1 = cor(corr[,c(1:9)])
p.mat <- cor_pmat(corr, use = "complete", method = 'pearson')
ggcorrplot(corr1,hc.order = TRUE, type = "lower", lab = TRUE, p.mat = p.mat, insig = "blank")
```
## 2/6 fig 1/3 table


Let's start by calculating the TB rate as the rate of TB cases per unit population:

```{r}
library(ggstatsplot)
library(ggpubr) 
# risk is defined as the rate of TB cases per unit population
TBdata$TB_Rating <- TBdata$TB / TBdata$Population
# Histograms for each of the two variables so that their distribution can be observed
p1<-ggscatterstats(data = TBdata, x = "Indigenous", y = "TB_Rating")
p2<-ggscatterstats(data = TBdata, x = "Illiteracy", y = "TB_Rating")
p3<-ggscatterstats(data = TBdata, x = "Urbanisation", y = "TB_Rating")
p4<-ggscatterstats(data = TBdata, x = "Density", y = "TB_Rating")
p5<-ggscatterstats(data = TBdata, x = "Poverty", y = "TB_Rating")
p6<-ggscatterstats(data = TBdata, x = "Poor_Sanitation", y = "TB_Rating")
p7<-ggscatterstats(data = TBdata, x = "Unemployment", y = "TB_Rating")
p8<-ggscatterstats(data = TBdata, x = "Timeliness", y = "TB_Rating")
ggarrange(p1, p2, p3, p4, p5, p6, p7, p8, nrow =3, ncol =3)
```

Looking at these scatterplots, you can see the relationship between the independent and dependent variablesï¼š 1.Higher levels of Indigenous status appear to be associated with higher rates of TB risk. 2.Poverty,Urbanisation and Poor_Sanitation also appear to be correlated with TB risk Because these scatter plots are only between univariate and TB_Rating, confounding variables may not be considered, so we need further analysis to determine

```{r}
TBdata_train <- TBdata %>% filter(Year < 2014)
```

## this is little note about why I use offset(log(Population)) 
##In the context of a Generalized Additive Model (GAM) or a Generalized Linear Model (GLM), the offset term is used to account for a known relationship between the response variable and an independent variable. In this case, the offset(log(Population)) term is added to the model to account for the relationship between TB cases and the population size.

##Including an offset allows the model to focus on the relationship between the response variable (TB cases) and the other independent variables, while adjusting for the population size. The log(Population) part of the offset term represents the natural logarithm of the population size, which is a common transformation used when dealing with count data, such as the number of TB cases.

##By including the offset term in the model, we are essentially modeling the rate of TB cases per unit population, rather than the absolute number of cases. This helps to control for differences in population size across microregions, allowing for a more meaningful comparison of TB risk between different areas.

```{r}
mod1 = gam(TB ~ offset(log(Population))+ s(Illiteracy, k = 10, bs = "cr") + s(Indigenous, k = 10, bs = "cr") + s(Urbanisation, k = 10, bs = "cr") +
              s(Density, k = 10, bs = "cr") + Poverty + s(Poor_Sanitation, k = 10, bs = "cr") + Unemployment + Timeliness +
              as.factor(Year) + Region + s(lon, lat), 
              data = TBdata_train, 
              family = nb(link = 'log'))
```

```{r}
# Fit the GAM
mod2 = gam(TB ~ offset(log(Population))+ s(Illiteracy, k = 10, bs = "cr") + s(Indigenous, k = 10, bs = "cr") + s(Urbanisation, k = 10, bs = "cr") +
              s(Density, k = 10, bs = "cr") + Poverty + s(Poor_Sanitation, k = 10, bs = "cr") + Unemployment + Timeliness +
              as.factor(Year) + Region + s(lon, lat), 
              data = TBdata_train, 
              family = poisson)

```


```{r}
# Fit the GAM
mod3 = gam(TB ~ offset(log(Population))+ s(Illiteracy, k = 10, bs = "cr") + s(Indigenous, k = 10, bs = "cr") + s(Urbanisation, k = 10, bs = "cr") +
              s(Density, k = 10, bs = "cr") + Poverty + s(Poor_Sanitation, k = 10, bs = "cr") + Unemployment + Timeliness +
              as.factor(Year) + Region + s(lon, lat), 
              data = TBdata_train, 
              family = gaussian(link = "identity"))

```



## 1/6 fig 2/3 table

```{r}
# Summarise the model
summary(mod1)
summary(mod2)
summary(mod3)
```


## 2/6 fig 2/3 table
## you can find example from MTHM506/COMM511 - Statistical Data Modelling Topic 3 - An example of GAMs page 8


```{r}
# 2x2 plot for the residuals
par(mfrow=c(2,2))
#  Runing gam.check on our original model
gam.check(mod1,pch=20)
gam.check(mod2,pch=20)
gam.check(mod3,pch=20)
```

```{r}
AIC(mod1)
AIC(mod2)
AIC(mod3)
```

### AIC from mod1 is the lowest choose mod1 and try to remove an item from model in our report we need to explan and provide evidence about removing parts of our model doesn't affect the model fit such as I remove Year and add spine funtion with k =10 to Poverty cause they are the low score in Pr(>|z|)


```{r}
mod4 = gam(TB ~ offset(log(Population))+ s(Illiteracy, k = 10, bs = "cr") + s(Indigenous, k = 10, bs = "cr") + s(Urbanisation, k = 10, bs = "cr") +
              s(Density, k = 10, bs = "cr")+ s(Poverty, k = 10, bs = "cr") + s(Poor_Sanitation, k = 10, bs = "cr") + s(Unemployment, k = 10, bs = "cr") + s(Timeliness, k = 10, bs = "cr") + Region + s(lon, lat), 
              data = TBdata_train, 
              family = nb(link = 'log'))
```


```{r}
# Summarise the model
summary(mod4)
summary(mod1)
```
This table presents the results of two Generalized Additive Models (GAMs) fitted to predict the number of TB cases using different socio-economic covariates, spatial information, and regional information. Both models use a negative binomial family with a log link function, which is appropriate for count data with overdispersion.

The first GAM model uses smooth terms (non-linear functions) for all socio-economic covariates, and the second GAM model uses smooth terms for some covariates while keeping Poverty, Unemployment, and Timeliness as linear terms. The spatial information (longitude and latitude) is also included in both models as a smooth term.

The table provides the following information for each GAM model:

Parametric coefficients: These are the coefficients for the linear terms in the model. For the first model, only the intercept and the Region variable are linear. In the second model, Poverty, Unemployment, and Timeliness are also linear terms. The coefficients, standard errors, z-values, and p-values are provided for each term.
Approximate significance of smooth terms: These are the results for the non-linear smooth terms in the model. The table shows the estimated degrees of freedom (edf), which represents the flexibility of the smooth term, the reference degrees of freedom (Ref.df), the Chi-squared test statistic (Chi.sq), and the p-value for each smooth term. The p-values help to determine the significance of the smooth terms in the model.
Model performance metrics: For both models, the adjusted R-squared (R-sq.(adj)) and the percentage of deviance explained are provided, which can be used to compare the models' performance. The model with the higher R-squared and deviance explained is generally considered a better fit for the data.
In summary, this table presents the results of two GAM models for predicting TB cases using socio-economic, spatial, and regional information. The table includes the coefficients for linear terms, the significance of non-linear smooth terms, and model performance metrics. These models can help understand the relationships between the predictors and the number of TB cases and can be useful for identifying high-risk areas and factors associated with the prevalence of TB.

## 3/6 fig 2/3 table explain this 4 graph below 
## you can find example from MTHM506/COMM511 - Statistical Data Modelling Topic 3 - An example of GAMs page 8

```{r}
# 2x2 plot for the residuals
par(mfrow=c(2,2))
#  Runing gam.check on our original model
gam.check(mod4,pch=20)
gam.check(mod1,pch=20)
```

## After changing mod4 better than mod1

```{r}
AIC(mod4)
AIC(mod1)
```


## plot map using mod4 on traning dataset

## 4/6 fig 2/3 table

```{r}
predicted_TB_Risk <- predict(mod4, newdata = TBdata_train, type = "response")
plot.map(x = predicted_TB_Risk, main = "Predicted TB Risk", n.levels = 7, cex = 1)
```

## 5/6 fig 2/3 table this is 2014 we use this to validate our model

```{r}
# PLotting map of cases
plot.map(TBdata$TB[TBdata$Year==2014],n.levels=7,main="TB counts for 2014")
```


## After compare in TB training data (year between 2012 and 2013) and test data 2014 model is "........"

## Then we use model to help The health authorities to allocate resources for hospitals to cope with the TB cases, so they would like to know if there are regions where the rate of TB per unit population is high and where you would recommend allocating these resources.


```{r}
#final model
modf = gam(TB ~ offset(log(Population))+ s(Illiteracy, k = 10, bs = "cr") + s(Indigenous, k = 10, bs = "cr") + s(Urbanisation, k = 10, bs = "cr") +
              s(Density, k = 10, bs = "cr")+ s(Poverty, k = 10, bs = "cr") + s(Poor_Sanitation, k = 10, bs = "cr") + s(Unemployment, k = 10, bs = "cr") + s(Timeliness, k = 10, bs = "cr") + Region + s(lon, lat), 
              data = TBdata, 
              family = nb(link = 'log'))

```


## 5/6 fig 3/3 table 

```{r}
# Identify regions with high TB risk
pred <- predict(modf, type="response")
TBdata$risk <- pred / TBdata$Population   # calculate TB risk
high_risk <- subset(TBdata, risk > quantile(risk, 0.95))   # regions with top 5% TB risk
high_risk
```
## 6/6 fig 3/3 table location on Brazil map

```{r}
# PLotting map of cases
predicted_TB_Risk <- predict(modf, newdata = TBdata, type = "response")
plot.map(x = predicted_TB_Risk, main = "Predicted TB Risk", n.levels = 7, cex = 1)
```

